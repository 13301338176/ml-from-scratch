{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Neural Networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start from a simple NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# logic formula: (A & B) | (B ^ C)\n",
    "X = np.asarray([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0],\n",
    "    [1, 1, 1],\n",
    "])\n",
    "\n",
    "y = np.asarray([\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [0],\n",
    "    [0],\n",
    "    [1],\n",
    "    [1],\n",
    "    [1],\n",
    "])\n",
    "\n",
    "def sigmoid(x, derivative = False):\n",
    "    \"\"\"\n",
    "    Compute the sigmoid of x\n",
    "    \"\"\"\n",
    "    if not derivative:\n",
    "        s = 1 / (1 + np.exp(-x))\n",
    "    else:\n",
    "        s = sigmoid(x) * sigmoid(1 - x)\n",
    "    return s\n",
    "def initialize_parameters(x_dim, h_dim, y_dim):\n",
    "\n",
    "    l1_size = h_dim\n",
    "    W1 = np.random.randn(l1_size, x_dim)\n",
    "    b1 = np.zeros((l1_size, 1))\n",
    "    \n",
    "    W2 = np.random.randn(y_dim, l1_size)\n",
    "    b2 = np.zeros((y_dim, 1))\n",
    "    \n",
    "    parameters = {\n",
    "        \"W1\": W1,\n",
    "        \"b1\": b1,\n",
    "        \"W2\": W2,\n",
    "        \"b2\": b2\n",
    "    }\n",
    "    return parameters\n",
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n",
    "    Z1 = np.matmul(W1, X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.matmul(W2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    \n",
    "    mem = {\"Z1\": Z1,\n",
    "           \"A1\": A1,\n",
    "           \"Z2\": Z2,\n",
    "           \"A2\": A2\n",
    "        }\n",
    "    \n",
    "    return A2, mem\n",
    "\n",
    "def compute_cost(A2, x, y):\n",
    "    \n",
    "    m = x.shape[0]\n",
    "    errors = - 1/m * (y.T.dot(np.log(sigmoid(A2))) + (1-y).T.dot(sigmoid(1-A2)))\n",
    "    cost = np.squeeze(np.sum(errors))\n",
    "    return cost\n",
    "    \n",
    "\n",
    "def backward_propagation(parameters, mem, X, Y):\n",
    "    W1, b1, W2, b2 = parameters[\"W1\"], parameters[\"b1\"], parameters[\"W2\"], parameters[\"b2\"]\n",
    "    Z1, A1, Z2, A2 = mem[\"Z1\"], mem[\"A1\"], mem[\"Z2\"], mem[\"A2\"]\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    dZ2 = A2-Y\n",
    "    dW2 = 1/m*np.dot(dZ2,A1.T)\n",
    "    db2 = 1/m* np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dZ1 = np.dot(W2.T,dZ2) * (1-np.power(A1,2))\n",
    "    dW1 = 1/m*np.dot(dZ1,X.T)\n",
    "    db1 = 1/m*np.sum(dZ1,axis=1,keepdims=True)\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads \n",
    "    \n",
    "    \n",
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    W1 = W1-learning_rate*dW1\n",
    "    b1 = b1-learning_rate*db1\n",
    "    W2 = W2-learning_rate*dW2\n",
    "    b2 = b2-learning_rate*db2\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters\n",
    "\n",
    "\n",
    "def nn_model(X, Y, n_h, num_iterations = 10000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "        n_x = layer_sizes(X, Y)[0]\n",
    "    n_y = layer_sizes(X, Y)[2]\n",
    "    \n",
    "    # Initialize parameters, then retrieve W1, b1, W2, b2. Inputs: \"n_x, n_h, n_y\". Outputs = \"W1, b1, W2, b2, parameters\".\n",
    "    parameters = initialize_parameters(n_x,n_h,n_y)\n",
    "    W1 = parameters[\"W1\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        A2, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y, parameters\". Outputs: \"cost\".\n",
    "        cost = compute_cost(A2, Y, parameters)\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        grads = backward_propagation(parameters,cache,X,Y)\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        parameters = update_parameters(parameters, grads)\n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" %(i, cost))\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer_sizes' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-a08f90a2c97d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mparameters\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-6-8c6bd896c1b6>\u001b[0m in \u001b[0;36mnn_model\u001b[0;34m(X, Y, n_h, num_iterations, print_cost)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m     \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m     \u001b[0mn_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_sizes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    141\u001b[0m     \u001b[0mn_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_sizes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'layer_sizes' is not defined"
     ],
     "output_type": "error"
    }
   ],
   "source": [
    "parameters = nn_model(X, y, 4, num_iterations=10000, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
